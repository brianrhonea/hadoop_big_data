{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Project Step 2\n",
    "###B. Stock Market Data Analysis\n",
    "In this step you will use Spark RDDs to analyze and summarize the dataset you stored on HDFS in step\n",
    "1.B of the previous project. Use RDDs and their suitable operations to provided the following summaries.\n",
    "\n",
    "###Data Overview:\n",
    "-stock: This column represents the stock symbol (e.g., AAPL for Apple, MSFT for Microsoft).\n",
    "\n",
    "-datetime: This column represents the date and time when the stock data was recorded.\n",
    "\n",
    "-volume: The number of shares traded during the given time period. This is a measure of the trading activity for a stock.\n",
    "\n",
    "-open: The opening price of the stock at the beginning of the time period.\n",
    "\n",
    "-high: The highest price at which the stock traded during the given time period.\n",
    "\n",
    "-low: The lowest price at which the stock traded during the given time period.\n",
    "\n",
    "-close: The closing price of the stock at the end of the time period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#path to hdfs\n",
    "path = '/training/flume/stock_csv.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the csv into a rdd\n",
    "stock_rdd = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'stock,datetime,volume,open,high,low,close',\n",
       " u'AAPL,2024-10-21 12:28:00,484,236.27000,236.27000,236.26000,236.26000',\n",
       " u'IBM,2024-10-21 12:25:00,120,231.53000,231.53000,231.53000,231.53000']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print intial rdd\n",
    "stock_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 rows after splitting by comma:\n",
      "[[u'AAPL', u'2024-10-21 12:28:00', u'484', u'236.27000', u'236.27000', u'236.26000', u'236.26000'], [u'IBM', u'2024-10-21 12:25:00', u'120', u'231.53000', u'231.53000', u'231.53000', u'231.53000'], [u'GOOGL', u'2024-10-21 12:28:00', u'346', u'163.53500', u'163.53500', u'163.50000', u'163.50000']]\n"
     ]
    }
   ],
   "source": [
    "header = stock_rdd.first()  # Get the first row (header)\n",
    "stock_rdd = stock_rdd.filter(lambda row: row != header)  # Remove the header\n",
    "stock_rddNH = stock_rdd.map(lambda line: line.split(\",\"))  # Split each row by comma\n",
    "\n",
    "\n",
    "print(\"First 3 rows after splitting by comma:\")\n",
    "print(stock_rddNH.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to safely convert a string to a float, return None if invalid\n",
    "def safe_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return None  # Return None if the value cannot be converted to a float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1) How many records are there in the table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in the table: 3110\n"
     ]
    }
   ],
   "source": [
    "total_records_rdd = stock_rddNH.count()\n",
    "print(\"Total records in the table: {}\".format(total_records_rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2) How many different days are there in the table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different days: 5\n"
     ]
    }
   ],
   "source": [
    "# 2. How many different days are there in the table?\n",
    "# Extract the date part from 'datetime' (assumes datetime format is 'YYYY-MM-DD HH:MM:SS')\n",
    "distinct_days_rdd = stock_rddNH.map(lambda row: row[1].split(\" \")[0]).distinct()\n",
    "print(\"Number of different days: {}\".format(distinct_days_rdd.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3. How many records per each day are there in the table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Records Per Day: [(u'2024-10-30', 75), (u'2024-10-24', 800), (u'2024-10-23', 635), (u'2024-10-22', 800), (u'2024-10-21', 800)]\n"
     ]
    }
   ],
   "source": [
    "# 3. How many records per each day are there in the table?\n",
    "records_per_day_rdd = stock_rddNH.map(lambda row: (row[1].split(\" \")[0], 1)) \\\n",
    "                                 .reduceByKey(lambda a, b: a + b)\n",
    "print(\"Count of Records Per Day: {}\".format(records_per_day_rdd.take(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4. What are the symbols in the table?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 symbols in the table: [u'MSFT', u'AAPL', u'IBM']\n"
     ]
    }
   ],
   "source": [
    "# 4. What are the symbols in the table?\n",
    "symbols_rdd = stock_rddNH.map(lambda row: row[0]).distinct()\n",
    "print(\"First 3 symbols in the table: {}\".format(symbols_rdd.take(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5. What is the highest price for each symbol?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 highest prices per symbol: [(u'MSFT', 434.91), (u'AAPL', 236.8), (u'IBM', 233.325)]\n"
     ]
    }
   ],
   "source": [
    "# 5. What is the highest price for each symbol?\n",
    "price_per_symbol_rdd = stock_rddNH.map(lambda row: (row[0], safe_float(row[4])))  # Extract (symbol, high price)\n",
    "price_per_symbol_rdd = price_per_symbol_rdd.filter(lambda x: x[1] is not None)  # Filter invalid entries\n",
    "highest_price_per_symbol_rdd = price_per_symbol_rdd.reduceByKey(lambda a, b: max(a, b))\n",
    "print(\"First 3 highest prices per symbol: {}\".format(highest_price_per_symbol_rdd.take(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6. What is the lowest price for each symbol?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 lowest prices per symbol: [(u'MSFT', 414.92), (u'AAPL', 228.07), (u'IBM', 204.63)]\n"
     ]
    }
   ],
   "source": [
    "# 6. What is the lowest price for each symbol?\n",
    "lowest_price_per_symbol_rdd = price_per_symbol_rdd.reduceByKey(lambda a, b: min(a, b))\n",
    "print(\"First 3 lowest prices per symbol: {}\".format(lowest_price_per_symbol_rdd.take(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##7. What is the average price for each symbol?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average price for each symbol:', [(u'MSFT', 1514.8585209003215), (u'AAPL', 3041.279742765273), (u'IBM', 739.1832797427653)])\n"
     ]
    }
   ],
   "source": [
    "# Sum up the prices and count them for each symbol, then divide for average\n",
    "avg_price_rdd = stock_rddNH.map(lambda row: (row[0], (float(row[2]), 1)))  # Map to (symbol, (price, count))\n",
    "total_and_count_rdd = avg_price_rdd.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))  # Sum prices and counts\n",
    "avg_price_per_symbol = total_and_count_rdd.mapValues(lambda x: x[0] / x[1])  # Calculate average\n",
    "print(\"Average price for each symbol:\", avg_price_per_symbol.take(3))  # Print first 3 symbols with average price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##8. What is the range of price for each symbol?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 price ranges per symbol: [(u'MSFT', 19.99000000000001), (u'AAPL', 8.740000000000009), (u'IBM', 28.814999999999998)]\n"
     ]
    }
   ],
   "source": [
    "# 8. What is the range of price for each symbol?\n",
    "# Range = highest price - lowest price\n",
    "price_range_per_symbol_rdd = stock_rddNH.map(lambda row: (row[0], (safe_float(row[4]), safe_float(row[5]))))  # (symbol, (high, low))\n",
    "price_range_per_symbol_rdd = price_range_per_symbol_rdd.filter(lambda x: x[1][0] is not None and x[1][1] is not None)\n",
    "min_max_price_rdd = price_range_per_symbol_rdd.reduceByKey(lambda a, b: (max(a[0], b[0]), min(a[1], b[1])))\n",
    "price_range_per_symbol_rdd = min_max_price_rdd.mapValues(lambda x: x[0] - x[1])  # Range = max - min\n",
    "print(\"First 3 price ranges per symbol: {}\".format(price_range_per_symbol_rdd.take(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9. What is the date on which each symbol experienced the highest price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 dates of highest prices per symbol: [(u'MSFT', (434.91, u'2024-10-30 15:46:00')), (u'AAPL', (236.8, u'2024-10-21 14:04:00')), (u'IBM', (233.325, u'2024-10-23 15:56:00'))]\n"
     ]
    }
   ],
   "source": [
    "# 9. What is the date on which each symbol experienced the highest price?\n",
    "# We need to find the date corresponding to the highest price for each symbol\n",
    "highest_price_date_per_symbol_rdd = stock_rddNH.map(lambda row: (row[0], (safe_float(row[4]), row[1])))  # (symbol, (high, date))\n",
    "highest_price_date_per_symbol_rdd = highest_price_date_per_symbol_rdd.filter(lambda x: x[1][0] is not None)  # Filter invalid entries\n",
    "highest_price_date_per_symbol_rdd = highest_price_date_per_symbol_rdd.reduceByKey(lambda a, b: a if a[0] > b[0] else b)\n",
    "print(\"First 3 dates of highest prices per symbol: {}\".format(highest_price_date_per_symbol_rdd.take(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 3. Part B\n",
    "In this step I will use Spark SQL to perform analysis on the data you transferred to Hadoop in the\n",
    "previous project. This means that you should only use Spark Dataframes and their operations to find the\n",
    "answers to the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stock: string (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- volume: integer (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      "\n",
      "stock datetime             volume open    high    low     close  \n",
      "AAPL  2024-10-21 12:28:... 484    236.27  236.27  236.26  236.26 \n",
      "IBM   2024-10-21 12:25:... 120    231.53  231.53  231.53  231.53 \n",
      "GOOGL 2024-10-21 12:28:... 346    163.535 163.535 163.5   163.5  \n",
      "MSFT  2024-10-21 12:27:... 456    415.16  415.16  415.06  415.06 \n",
      "TSLA  2024-10-21 12:28:... 732    218.56  218.58  218.52  218.58 \n",
      "AAPL  2024-10-22 14:54:... 1097   235.325 235.35  235.32  235.325\n",
      "IBM   2024-10-22 14:53:... 333    231.78  231.78  231.77  231.77 \n",
      "GOOGL 2024-10-22 14:54:... 102    164.69  164.69  164.69  164.69 \n",
      "MSFT  2024-10-22 14:54:... 590    429.27  429.27  429.19  429.19 \n",
      "TSLA  2024-10-22 14:54:... 441    217.26  217.335 217.26  217.325\n",
      "AAPL  2024-10-24 11:26:... 1247   229.85  229.94  229.85  229.89 \n",
      "IBM   2024-10-24 11:25:... 424    218.47  218.47  218.225 218.225\n",
      "GOOGL 2024-10-24 11:25:... 1083   162.41  162.43  162.37  162.37 \n",
      "MSFT  2024-10-24 11:25:... 734    424.66  424.66  424.59  424.615\n",
      "TSLA  2024-10-24 11:25:... 4236   253.26  253.57  253.23  253.47 \n",
      "AAPL  2024-10-22 14:00:... 1335   235.605 235.625 235.57  235.57 \n",
      "IBM   2024-10-22 14:00:... 100    232.37  232.37  232.37  232.37 \n",
      "GOOGL 2024-10-22 14:00:... 605    165.04  165.04  165.03  165.03 \n",
      "MSFT  2024-10-22 14:00:... 955    429.33  429.33  429.25  429.25 \n",
      "TSLA  2024-10-22 13:59:... 589    217.42  217.42  217.4   217.4  \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"stock\", StringType(), True),\n",
    "    StructField(\"datetime\", TimestampType(), True), \n",
    "    StructField(\"volume\", IntegerType(), True),\n",
    "    StructField(\"open\", DoubleType(), True),\n",
    "    StructField(\"high\", DoubleType(), True),\n",
    "    StructField(\"low\", DoubleType(), True),\n",
    "    StructField(\"close\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Load the CSV file as RDD\n",
    "file_path = \"/training/flume/stock_csv.csv\"  # Replace with your HDFS path\n",
    "rdd = sc.textFile(file_path)\n",
    "\n",
    "# Extract header\n",
    "header = rdd.first()\n",
    "\n",
    "# Filter header and split lines\n",
    "data_rdd = rdd.filter(lambda line: line != header).map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Function to convert datetime string to Python datetime object\n",
    "def convert_to_datetime(datetime_str):\n",
    "    return datetime.strptime(datetime_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Explicitly cast each field to the correct type\n",
    "row_rdd = data_rdd.map(lambda fields: (\n",
    "    fields[0].encode(\"utf-8\"),  # stock (String)\n",
    "    convert_to_datetime(fields[1].encode(\"utf-8\")),  # datetime (Datetime)\n",
    "    int(fields[2]),             # volume (Integer)\n",
    "    float(fields[3]),           # open (Double)\n",
    "    float(fields[4]),           # high (Double)\n",
    "    float(fields[5]),           # low (Double)\n",
    "    float(fields[6])            # close (Double)\n",
    "))\n",
    "\n",
    "# Create DataFrame from RDD\n",
    "stock_df = sqlContext.createDataFrame(row_rdd, schema)\n",
    "\n",
    "# Register the DataFrame as a temporary SQL table\n",
    "stock_df.registerTempTable(\"stocks\")\n",
    "\n",
    "# Verify schema and data\n",
    "stock_df.printSchema()\n",
    "stock_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1) How many records are there in the table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_count\n",
      "3110        \n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT COUNT(*) AS record_count FROM stocks\").show()\n",
    "#This number is 1 off project 2 because in this project I dropped headers before creating table, whereas in project 2 I forgot to remove the headers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2) How many different days are there in the table?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct days: 5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Extract the date from the timestamp\n",
    "distinct_days = stock_df.select(col(\"datetime\").cast(\"date\")).distinct().count()\n",
    "print \"Distinct days: {}\".format(distinct_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3) How many records per each day are there in the table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       count\n",
      "2024-10-21 800  \n",
      "2024-10-22 800  \n",
      "2024-10-23 635  \n",
      "2024-10-24 800  \n",
      "2024-10-30 75   \n"
     ]
    }
   ],
   "source": [
    "# Group by the extracted date and count the records\n",
    "records_per_day = stock_df.withColumn(\"date\", col(\"datetime\").cast(\"date\")) \\\n",
    "    .groupBy(\"date\").count().orderBy(\"date\")\n",
    "records_per_day.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4) What are the symbols in the table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock\n",
      "AAPL \n",
      "GOOGL\n",
      "IBM  \n",
      "MSFT \n",
      "TSLA \n"
     ]
    }
   ],
   "source": [
    "# Get distinct symbols\n",
    "symbols = stock_df.select(\"stock\").distinct()\n",
    "symbols.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5) What is the highest price for each symbol?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock MAX(high#4)\n",
      "AAPL  236.8      \n",
      "GOOGL 175.66     \n",
      "IBM   233.325    \n",
      "MSFT  434.91     \n",
      "TSLA  259.43     \n"
     ]
    }
   ],
   "source": [
    "# Group by symbol and get the highest price for each symbol\n",
    "highest_price = stock_df.groupBy(\"stock\").agg({\"high\": \"max\"})\n",
    "highest_price.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6) What is the lowest price for each symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock MIN(low#5)\n",
      "AAPL  228.06    \n",
      "GOOGL 161.925   \n",
      "IBM   204.51    \n",
      "MSFT  414.92    \n",
      "TSLA  212.4     \n"
     ]
    }
   ],
   "source": [
    "# Group by symbol and get the lowest price for each\n",
    "lowest_price = stock_df.groupBy(\"stock\").agg({\"low\": \"min\"})\n",
    "lowest_price.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##7) What is the average price for each symbol?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock average_price     \n",
      "AAPL  233.3181752411574 \n",
      "GOOGL 163.89336816720268\n",
      "IBM   227.56491157556266\n",
      "MSFT  424.2107717041801 \n",
      "TSLA  227.69815916398719\n"
     ]
    }
   ],
   "source": [
    "# Group by symbol and get the average price for each symbol close\n",
    "average_price = sqlContext.sql(\"\"\"\n",
    "    SELECT stock, AVG(close) AS average_price\n",
    "    FROM stocks\n",
    "    GROUP BY stock\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##8) What is the range of price for each symbol?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol price_range       \n",
      "AAPL   8.719999999999999 \n",
      "GOOGL  13.689999999999998\n",
      "IBM    28.769999999999982\n",
      "MSFT   19.95999999999998 \n",
      "TSLA   46.94             \n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "    SELECT stock AS symbol, MAX(close) - MIN(close) AS price_range\n",
    "    FROM stocks\n",
    "    GROUP BY stock\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9) What is the date on which each symbol experienced the highest price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock event_date           high   \n",
      "AAPL  2024-10-21 14:04:... 236.8  \n",
      "GOOGL 2024-10-30 15:43:... 175.66 \n",
      "GOOGL 2024-10-30 15:44:... 175.66 \n",
      "IBM   2024-10-23 15:56:... 233.325\n",
      "MSFT  2024-10-30 15:47:... 434.91 \n",
      "MSFT  2024-10-30 15:46:... 434.91 \n",
      "TSLA  2024-10-30 15:45:... 259.43 \n",
      "TSLA  2024-10-30 15:49:... 259.43 \n",
      "AAPL\n"
     ]
    }
   ],
   "source": [
    "query = sqlContext.sql(\"\"\"\n",
    "    SELECT DISTINCT s.stock, s.datetime AS event_date, s.high\n",
    "    FROM stocks s\n",
    "    INNER JOIN (\n",
    "        SELECT stock, MAX(high) AS max_high\n",
    "        FROM stocks\n",
    "        GROUP BY stock\n",
    "    ) max_prices\n",
    "    ON s.stock = max_prices.stock\n",
    "    AND s.high = max_prices.max_high\n",
    "    ORDER BY stock ASC\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "query.show()\n",
    "\n",
    "# Collect the first row\n",
    "first_result = query.collect()[0][0]\n",
    "\n",
    "print(first_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
